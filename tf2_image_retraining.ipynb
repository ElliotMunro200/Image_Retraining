{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TF Hub for TF2: Retraining an image classifier",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "ScitaPqhKtuW"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:21:51.883688Z",
          "iopub.status.busy": "2020-10-02T12:21:51.883038Z",
          "iopub.status.idle": "2020-10-02T12:21:51.885374Z",
          "shell.execute_reply": "2020-10-02T12:21:51.884907Z"
        },
        "id": "RaJW3XrPyFiF"
      },
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:21:51.890616Z",
          "iopub.status.busy": "2020-10-02T12:21:51.889993Z",
          "iopub.status.idle": "2020-10-02T12:21:54.727906Z",
          "shell.execute_reply": "2020-10-02T12:21:54.727368Z"
        },
        "id": "50FYNIb1dmJH"
      },
      "source": [
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes,\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:21:54.741257Z",
          "iopub.status.busy": "2020-10-02T12:21:54.740565Z",
          "iopub.status.idle": "2020-10-02T12:21:54.749553Z",
          "shell.execute_reply": "2020-10-02T12:21:54.749987Z"
        },
        "id": "9f3yBUvkd_VJ"
      },
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:21:54.755068Z",
          "iopub.status.busy": "2020-10-02T12:21:54.754374Z",
          "iopub.status.idle": "2020-10-02T12:23:14.114283Z",
          "shell.execute_reply": "2020-10-02T12:23:14.114693Z"
        },
        "id": "w_YKX2Qnfg6x"
      },
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:23:14.135577Z",
          "iopub.status.busy": "2020-10-02T12:23:14.134856Z",
          "iopub.status.idle": "2020-10-02T12:23:14.388986Z",
          "shell.execute_reply": "2020-10-02T12:23:14.389403Z"
        },
        "id": "CYOw0fTO1W4x"
      },
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ8DKKgeKv4-"
      },
      "source": [
        "Try out the model on an image from the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:23:14.396252Z",
          "iopub.status.busy": "2020-10-02T12:23:14.395509Z",
          "iopub.status.idle": "2020-10-02T12:23:15.177358Z",
          "shell.execute_reply": "2020-10-02T12:23:15.176868Z"
        },
        "id": "oi1iCNB9K1Ai"
      },
      "source": [
        "def get_class_string_from_index(index):\n",
        "   for class_string, class_index in valid_generator.class_indices.items():\n",
        "      if class_index == index:\n",
        "         return class_string\n",
        "\n",
        "x, y = next(valid_generator)\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + get_class_string_from_index(true_index))\n",
        "print(\"Predicted label: \" + get_class_string_from_index(predicted_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCsAsQM1IRvA"
      },
      "source": [
        "Finally, the trained model can be saved for deployment to TF Serving or TF Lite (on mobile) as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-02T12:23:15.186147Z",
          "iopub.status.busy": "2020-10-02T12:23:15.183004Z",
          "iopub.status.idle": "2020-10-02T12:23:19.849887Z",
          "shell.execute_reply": "2020-10-02T12:23:19.854365Z"
        },
        "id": "LGvTi69oIc2d"
      },
      "source": [
        "saved_model_path = \"/tmp/saved_flowers_model\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzW4oNRjILaq"
      },
      "source": [
        "## Optional: Deployment to TensorFlow Lite\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite) lets you deploy TensorFlow models to mobile and IoT devices. The code below shows how to convert the trained model to TF Lite and apply post-training tools from the [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization). Finally, it runs it in the TF Lite Interpreter to examine the resulting quality\n",
        "\n",
        "  * Converting without optimization provides the same results as before (up to roundoff error).\n",
        "  * Converting with optimization without any data quantizes the model weights to 8 bits, but inference still uses floating-point computation for the neural network activations. This reduces model size almost by a factor of 4 and improves CPU latency on mobile devices.\n",
        "  * On top, computation of the neural network activations can be quantized to 8-bit integers as well if a small reference dataset is provided to calibrate the quantization range. On a mobile device, this accelerates inference further and makes it possible to run on accelerators like EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va1Vo92fSyV6"
      },
      "source": [
        "#@title Optimization settings\n",
        "# docs_infra: no_execute\n",
        "# TODO(b/156102192)\n",
        "optimize_lite_model = False  #@param {type:\"boolean\"}\n",
        "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
        "num_calibration_examples = 60  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "representative_dataset = None\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "if optimize_lite_model:\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "lite_model_content = converter.convert()\n",
        "\n",
        "with open(\"/tmp/lite_flowers_model\", \"wb\") as f:\n",
        "  f.write(lite_model_content)\n",
        "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wqEmD0xIqeG"
      },
      "source": [
        "# docs_infra: no_execute\n",
        "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
        "def lite_model(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMMK-fZrKrk8"
      },
      "source": [
        "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
        "# docs_infra: no_execute\n",
        "num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
        "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                for batch in train_generator\n",
        "                for (image, label) in zip(*batch))\n",
        "count = 0\n",
        "count_lite_tf_agree = 0\n",
        "count_lite_correct = 0\n",
        "for image, label in eval_dataset:\n",
        "  probs_lite = lite_model(image[None, ...])[0]\n",
        "  probs_tf = model(image[None, ...]).numpy()[0]\n",
        "  y_lite = np.argmax(probs_lite)\n",
        "  y_tf = np.argmax(probs_tf)\n",
        "  y_true = np.argmax(label)\n",
        "  count +=1\n",
        "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "  if y_lite == y_true: count_lite_correct += 1\n",
        "  if count >= num_eval_examples: break\n",
        "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}